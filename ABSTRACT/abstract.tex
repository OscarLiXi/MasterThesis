%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.

Mobile robot navigation has been an attractive topic in the Robotics area for a long time. \textit{Visual SLAM} simultaneously locates robots' poses and constructs a map via analysing measurements obtained from visual sensors. However due to the costly computational complexity of camera, it is non-trivial to gain a high-quality pose estimation while the whole system runs in real-time. \textit{Inertial Measurement Unit} (IMU) measures the rotational rate and acceleration of the robot under the higher output frequency, and it has been considered as an appropriate compensation to a single camera in such navigation systems. In this master thesis, we aim to exploit a visual-inertial odometry, to further improve localization results by fusing visual data and IMU data. More precisely, we apply a loosely-coupled visual-inertial integration method to keep our system runs in a constant time. 

We first discuss the types of visual SLAM. Generally, visual SLAM has two traditional approaches, which are filter-based methods and keyframe-based methods respectively. Filter-based methods marginalise all the previous camera poses to obtain the current pose estimation as well as optimized landmarks, whereas keyframe-based methods keep some of former poses and applies bundle adjustment to further improve the estimation results. We finally choose keyframe-based visual SLAM to be correction data in our Kalman filter framework because it is more efficient when scale of scene becomes larger, another reason is that it can handle loop closures more easily. We then discuss the way to integrate IMU data. We first learn \textit{quaternion} and its operations. Instead of using Euler angle (Gimbal Lock) or rotational matrix (high memory cost), we formulate the system rotation parameters with quaternion. Followed by that, we explain the time-integration and differentiations related to the quaternions, and we suggest a error-state Kalman filter to propagate system states while tracking the system uncertainties. In order to fuse the visual data and IMU data, we suggest an adapted map scale method to automatically propagate the map scale factor, therefore solve the unknown map scale problem existed in mono visual SLAM. A keyframe-based bundle adjustment is then exploited for improving the tracking quality. Our experiments show that our suggested framework obtain more accurate and stable results compared to the state-of-art visual odometry, and our system can be easily extended to large scaled scene.

In addition to Kalman filter based framework, we briefly introduce a manifold optimization approach to solve visual-inertial odometry problem. We have suggested a standard way to solve manifold optimization as well as several ideas to reduce the computational time in the end.

