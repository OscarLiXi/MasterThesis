%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Modular Sensor Fusing}
\label{chap:sensor_fusing}

In the previous chapters, we learned how to represent each frame and analysed the difference between filter method and keyframe BA method for our odometry system; we also learned quaternion algebra and basic approaches for quaternion integration and derivative over the time under some general assumptions. In this chapter, we will explore the details of our sensor fusing approach, which uses so called \textit{IMU loosely-coupled integration framework}~\cite{weiss2012vision}. In such a framework, system propagates states via Kalman filter (KF) based on IMU measurements, and extrasensory (\eg, camera, GPS \etc) data are used in correction step. The computational cost for Kalman-filter-styled approach is usually linear, hence IMU loosely-coupled integration framework provides a better trade-off between computational complexity and accuracy in a real-time robotic navigation system.

\section{Error-state Kalman Filter for IMU Integration}
\label{sec:ESKF_IMU}

The error-state Kalman filter (ESKF) follows the paradigm of Kalman filter, which also has prediction step and correction step. However, ESKF separates system into three different states: true state, nominal state, and error state. Nominal state processes large signal, which is integrable in a non-linear fashion, whereas error state keeps track of error and noise term, which can be integrated in a linear way. The composition of nominal state and error state is called true state, which is the realistic status of the system.

The ESKF has several beneficial properties when building a visual-inertial odometry:                                                                                                                                                                                                                                                                                                              
\begin{enumerate}
	\item The computation of Jacobian is often very fast, because the error state is small and all second order products are negligible. This is an important factor for building a real-time system.
	\item Fusing visual data with IMU data is straightforward in Kalman filter correction step. One can utilize the tracking result to correct the IMU integration state.
	\item Large signals have been integrated into nominal states, so that we can apply the correction step in a lower rate than prediction step.
\end{enumerate}

The procedure of ESKF in this system can be explained as follows. Firstly, IMU data is integrated into nominal state via numerical integration methods, note that nominal state does not take noise terms or error terms into account, hence nominal state will accumulate errors continuously. The error state then predicts the errors and noise terms using general extended KF paradigm, meaning it will predict the mean and covariance of the error state. In parallel a correction step is performed at a lower rate, the results of visual tracking are used to correct the error state, the error state is then injected into nominal state, which the nominal state becomes a final estimation of system at that point. The system continues until the termination conditions have been met.

We explain the ESKF for IMU integration in this section, the derivations are partially based on the instructions in \cite{Quaternion2015Joan}. Visual sensor as the correction data is introduced in Section \ref{sec:camera_comple_data}, which is inspired by \cite{weiss2012vision}.

%\subsection{Motivation}
%\label{subsec:ESKF_IMU_sub1}

\subsection{System Kinematics}
\label{subsec:ESKF_IMU_sub2}

We denote our true state $\vec{x}_t$ as
\begin{equation}\label{f1}
	\vec{x}_t = \vec{x}_n \oplus \vec{x}_e,
\end{equation}
where $\vec{x}_n$ is the nominal state for large signals, $\vec{x}_e$ is error state for small error/noise signal, and we have used $\oplus$ to denote a general composition step. 

We then introduce position $\vec{p}$, velocity $\vec{v}$, quaternion $\vec{q}$, accelerometer bias $\vec{a}_b$, gyroscope bias $\vec{\omega}_b$ and gravity vector $\vec{g}$ into true state, nominal state and error state respectively. The general composition step can be shown as
\begin{align}
	\vec{p}_t =& \vec{p}_n + \vec{p}_e \\
	\vec{v}_t =& \vec{v}_n + \vec{v}_e \\
	\vec{q}_t \approx& \vec{q}_n \otimes \begin{bmatrix} 1 \\ \vec{\theta}_e / 2 \end{bmatrix} \label{f2}\\
	\vec{a}_{bt} =& \vec{a}_{bn} + \vec{a}_{be} \\
	\vec{\omega}_{bt} =& \vec{\omega}_{bn} + \vec{\omega}_{be} \\ 
	\vec{g}_t =& \vec{g}_n + \vec{g}_e,
\end{align}
note that we derive Equation (\ref{f2}) by the small angle approximation (Equation (\ref{q30})). We apply angular error $\vec{\theta}_e$ instead of quaternion error in error state following classical approaches.

We then construct kinematic equations for true state, which are
\begin{align}
	\label{f18}
	\dot{\vec{p}_t} =& \vec{v}_t \\
	\dot{\vec{v}_t} =& \mR_t(\vec{a}_m - \vec{a}_{bt} - \vec{a}_n) + \vec{g}_t\\
	\dot{\vec{q}_t} =& \frac{1}{2}\vec{q}_t \otimes (\vec{\omega}_m - \vec{\omega}_{bt} - \vec{\omega}_n) \\
	\dot{\vec{a}_{bt}} =& \vec{a}_w \\
	\dot{\vec{\omega}_{bt}} =& \vec{\omega}_{w} \\ 
	\label{f19}
	\dot{\vec{g}_t} =& 0,
\end{align}
where $\vec{a}_m$ and $\vec{\omega}_m$ are the measurements from accelerometer and gyroscope respectively within \textbf{local frame}, $\vec{a}_n$ and $\vec{\omega}_n$ are noises with those measurements, $\vec{a}_w$ and $\vec{\omega}_w$ are white Gaussian noise together with accelerometer and gyroscope bias, and $\mR_t$ is the rotation matrix corresponding to true state quaternion, \ie, $\mR_t \triangleq \mR_t\{ \vec{q} \}$ regarding to Equation (\ref{q25}). We use similar notations in nominal state and error state. 

We obtain kinematic equations for nominal state by ignoring all small signals, which leads to
\begin{align}
	\label{f12}
	\dot{\vec{p}_n} =& \vec{v}_n \\
	\label{f11}
	\dot{\vec{v}_n} =& \mR_n(\vec{a}_m - \vec{a}_{bt}) + \vec{g}_n\\
	\dot{\vec{q}_n} =& \frac{1}{2}\vec{q}_n \otimes (\vec{\omega}_m - \vec{\omega}_{bn})\\
	\dot{\vec{a}_{bn}} =& 0 \\
	\dot{\vec{\omega}_{bn}} =& 0 \\ 
	\label{f13}
	\dot{\vec{g}_n} =& 0,
\end{align}
and the error state with small signals is
\begin{align}
	\label{f3}
	\dot{\vec{p}_e} =& \vec{v}_e \\
	\label{f4}
	\dot{\vec{v}_e} =& \mR_n \left[ \vec{a}_m - \vec{a}_{bn} \right]_{\times} -  \mR_n\vec{a}_{be} + \vec{g}_e - \mR_n\vec{a}_n\\
	\label{f5}
	\dot{\vec{\theta}_e} =& \left[ \vec{\omega}_m - \vec{\omega}_{bn} \right]_{\times} - \vec{\omega}_{be} - \vec{\omega}_{n}\\
	\label{f6}
	\dot{\vec{a}_{be}} =& \vec{a}_w \\
	\label{f7}
	\dot{\vec{\omega}_{be}} =& \vec{\omega}_{w} \\ 
	\label{f8}
	\dot{\vec{g}_e} =& 0. 
\end{align}
It is trivial to derive Equation (\ref{f3}, \ref{f6}, \ref{f7}, \ref{f8}), see Appendix \ref{chap:appendix1} for derivation of Equation (\ref{f4} and \ref{f5}).

\subsection{State Time-integration and Error-state Jacobian}
\label{subsec:ESKF_IMU_sub3}

We propose time-integration equations between any two time stamp $t_n$ and $t_{n+1}$ where we measure the time difference $\Delta{t}$ as $\Delta{t} = t_{n+1} - t_{n}$. In order to simplify our notations, we denote the last state parameters as $\vec{x}$, and denote the current state parameters as $\vec{x}^{\prime}$, where the current state is measured at time stamp $t_{n}$, and last state is measured at $t_{n-1}$. Same notations are set for error state. Therefore, time-integration equations for nominal state for one update are
\begin{align}
	\label{f112}
	\vec{p}_n^{\prime} =& \vec{p}_n + \vec{v}_n\Delta{t} + \frac{1}{2}(\mR_n(\vec{a}_m - \vec{a}_{bt}) + \vec{g}_n)\Delta{t}^2 \\
	\vec{v}_n^{\prime} =& \vec{v}_n + (\mR_n(\vec{a}_m - \vec{a}_{bt}) + \vec{g}_n)\Delta{t}\\
	\label{f9}
	\vec{q}_n^{\prime} =& \vec{q}_n \otimes \vec{q}\{(\vec{\omega}_m - \vec{\omega}_{bn})\Delta{t}\}\\
	\vec{a}_{bn}^{\prime} =& \vec{a}_{bn} \\
	\vec{\omega}_{bn}^{\prime} =& \vec{\omega}_{bn} \\ 
	\label{f113}
	\vec{g}_n^{\prime} =& \vec{g}_n.
\end{align}
We use \textbf{\textit{Zeroth order forward integration}} explained in Section \ref{sec:timei_on_quat} to integrate our state over time, this is also called (explicit) Euler method in Runge-Kutta numerical integration methods (see Appendix \ref{sec:runge_kutta}).

We integrate our error state in the same manner, except we have truncated second-order signal out. We obtain the integration equations for error state by
\begin{align}
	\vec{p}_e^{\prime} =& \vec{p}_e + \vec{v}_e \Delta{t}\\
	\vec{v}_e^{\prime} =& \vec{v}_e + (-\mR_n \left[ \vec{a}_m - \vec{a}_{bn} \right]_{\times}\vec{\theta}_e -  \mR_n\vec{a}_{be} + \vec{g}_e)\Delta{t} + \vec{v}_i\\
	\label{f10}
	\vec{\theta}_e^{\prime} =& (\mR_n^T \{ \vec{\omega}_m - \vec{\omega}_{bn} \} \vec{\theta}_e - \vec{\omega}_{be})\Delta{t} + \vec{\theta}_{i}\\
	\vec{a}_{be}^{\prime} =& \vec{a}_{be} + \vec{a}_i\\
	\vec{\omega}_{be}^{\prime} =& \vec{\omega}_{be} + \vec{\omega}_i\\ 
	\label{f114}
	\vec{g}_e^{\prime} =& \vec{g}_e,
\end{align}
where $\vec{v}_i$, $\vec{\theta}_{i}$, $\vec{a}_i$, and $\vec{\omega}_i$ are random impulses for velocity, angular error, accelerometer bias and gyroscope. Those impulses can be modelled by Gaussian process. We derive Equation (\ref{f10}) by close-formed integration methods described in Appendix \ref{sec:close_integration}.

We then provide the Jacobian of error state $\mJ_{x_e}$ for ESKF prediction step usage
\begin{equation}\label{f14}
	\mJ_{e^\prime e} = \frac{\partial{\vec{x}_e^{\prime}}}{\partial{\vec{x}_e }} = \begin{bmatrix}
	\ones & \ones\Delta{t} & 0 & 0 & 0 & 0 \\
	0 & \ones & \mR_n \left[ \vec{a}_m - \vec{a}_{bn} \right]_{\times}\Delta{t} & \mR_n\Delta{t} & 0 & \ones\Delta{t} \\
	0 & 0 & \mR_n^T \{ \vec{\omega}_m - \vec{\omega}_{bn} \}\Delta{t} & 0 & -\ones\Delta{t} & 0 \\
	0 & 0 & 0 & \ones & 0 & 0 \\
	0 & 0 & 0 & 0 & \ones & 0 \\
	0 & 0 & 0 & 0 & 0 & \ones \\
	\end{bmatrix}.
\end{equation}

Note that partial derivative between true state $\vec{x}_t$ and $\vec{x}_e$ is not identity because we use different parameters to represent orientations, \eg,  quaternions in true state and nominal state, angular errors in error state. We then give the Jacobian of true state with respect to error state with
\begin{equation}\label{f15}
	\mJ_{t e} = \frac{\partial{\vec{x}_t}}{\partial{\vec{x}_e }} = \begin{bmatrix}
	\ones & 0 & 0 & 0 & 0 & 0 \\
	0 & \ones &0 & 0 & 0 & 0 \\
	0 & 0 & \frac{\partial{\vec{q}_t}}{\partial{\vec{\theta}_e }} & 0 & 0 & 0 \\
	0 & 0 & 0 & \ones & 0 & 0 \\
	0 & 0 & 0 & 0 & \ones & 0 \\
	0 & 0 & 0 & 0 & 0 & \ones \\
	\end{bmatrix}.
\end{equation}
By Equation (\ref{q5}), we have
\begin{align}\label{f16}
	\frac{\partial{\vec{q}_t}}{\partial{\vec{\theta}_e }} &= \frac{1}{2}Q^+(\vec{q})\begin{bmatrix}
	0 & 0 & 0 \\
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & 0 & 1 \\
	\end{bmatrix} \\
	&= \frac{1}{2}\begin{bmatrix}
	-q_x & -q_y & -q_z \\
	q_w & -q_x & q_y \\
	q_z & q_w & -q_x \\
	-q_y & q_x & q_w \\
	\end{bmatrix}. 
\end{align}

\subsection{State Propagation}
\label{subsec:ESKF_IMU_sub4}

Initially, nominal state $\vec{x}_n$ has been set based on the prior knowledge, and there is no error at start, \ie, all terms in the error state are set to zero. We assume error state $\vec{x}_e$ as a normal distribution, \ie, $\vec{x}_e \sim \mathcal{N}(\hat{\vec{x}_e}, \mSigma)$, where $\mSigma$ denotes the covariance matrix for error state, which helps us to track the uncertainty of error state. Note that $\mSigma$ is initialized to a very small diagonal matrix.

At the certain round, we first obtain the measurements from our accelerator and gyroscope, and compute a new nominal state estimation $\hat{\vec{x}_n^\prime}$ by Equation (\ref{f112}) to (\ref{f113}). We then compute error state Jacobian $\mJ_{e^\prime e}^\prime$ by Equation (\ref{f14}), then update the error state and covariance matrix of current error state by
\begin{align} \label{f22}
\hat{\vec{x}_e^\prime} &= \mJ_{e^\prime e}^\prime\hat{\vec{x}_e} \\
\mSigma^\prime &= \mJ_{e^\prime e}^\prime \mSigma (\mJ_{e^\prime e}^\prime)^T,
\end{align}
which is called \textit{prediction step} in ESKF. We omit prime symbol in the next step, \ie, current state $\vec{x}$ is replaced by $\vec{x}^\prime$.

We then assume correction measurement $\vec{y}$ from extrasensory data is a non-linear function with additional white Gaussian noise $w \sim \mathcal{N}(0, \mW)$ of our true state, \ie,
\begin{equation}\label{f17}
	\vec{y} = h(\vec{x}_t) + w,
\end{equation}
and the \textit{correction step} of ESKF are as follows
\begin{align}\label{f30}
	\mK &= \mSigma\mH^T(\mH\mSigma\mH^T+\mW)^{-1} \\
	\hat{\vec{x}_e^\prime} &= \mK (\vec{y} - h(\hat{\vec{x}_t})) \\
	\mSigma^\prime &= (\ones - \mK\mH)\mSigma,
\end{align}
where $\mH$ is the Jacobian matrix of measurement function $h(\cdot)$ with respect to error state $\vec{x}_e$ (see Section \ref{subsec:camera_comple_data_sub2}). Note the estimation of true state here is the nominal state since we have not observed the mean of error state. The true state is estimated by Equation (\ref{f1}) and Equation (\ref{f18}) to (\ref{f19}). Depending on the output frequency of extra sensor, \textit{correction step} often happens on a lower rate than \textit{prediction step}. As always, we omit prime symbol in next few steps as state has been refreshed.  

Before the system enters into the next round, we reset error state to initial state, \ie, $\hat{\vec{x}_e} = 0$ in our case. We update the covariance matrix $\mSigma$ by
\begin{equation}\label{f20}
	\mSigma^\prime = \mJ_{ge} \mSigma (\mJ_{ge})^T,
\end{equation}
where $\mJ_{ge}$ is Jacobian matrix of updated error state with respect to old error state. $\mJ_{ge}$ is given by
\begin{equation}\label{f21}
	\mJ_{ge} \triangleq \dfrac{\partial{\vec{x}_e^\prime}}{\partial{\vec{x}_e}} = \begin{bmatrix}
	\ones_6 & 0 & 0 \\
	0 & \ones - \left[ \frac{1}{2} \hat{\vec{\theta}_e} \right]_\times & 0 \\
	0 & 0 & \ones_9 \\
	\end{bmatrix},
\end{equation}
where we use $\vec{x}^\prime$ to denote the new state after resetting error state. The Jacobian is identical on all diagonal blocks except angular error term. We derive the $\mJ_{ge}$ by two observations, first the true state will not be able to change during the reset of error state; second, the new nominal state is obtained by injecting the error state from last nominal state. These two observations can be transformed into following equations
\begin{align}
	\label{f31}
	\vec{x}_n^\prime \oplus \vec{x}_e^\prime =& \vec{x}_n \oplus \vec{x}_e \\
	\label{f32}
	\vec{x}_n^\prime =& \vec{x}_n \oplus \hat{\vec{x}_e}.
\end{align}
Putting Equation (\ref{f32}) into Equation (\ref{f31}), we have
\begin{equation}
	\vec{q}_e^\prime = (\vec{q}_n \otimes \hat{\vec{q}_e})^* \otimes \vec{q}_n \otimes \vec{q}_e = Q^+(\hat{\vec{q}_e})^*\vec{q}_e.
\end{equation}
By using small angle approximation (Equation (\ref{q5})) and ignoring the second-order term, we can obtain 
\begin{equation}
	\vec{\theta}_e^\prime = -\hat{(\vec{\theta}_e} + (\ones - \left[ \frac{1}{2} \hat{\vec{\theta}_e} \right]_\times)\vec{\theta}_e.
\end{equation}
After derivation of old angular term from both sides, we get 
\begin{equation}
	\dfrac{\partial{\vec{\theta}_e^\prime}}{\partial{\vec{\theta}_e}} = \ones - \left[ \frac{1}{2} \hat{\vec{\theta}_e} \right]_\times,
\end{equation}
which is precisely the term in Equation (\ref{f21}).
\section{Camera as Complementary Sensory Data}
\label{sec:camera_comple_data}

As we discussed in Section \ref{sec:ESKF_IMU}, we need extrasensory data in ESKF \textit{correction step}, and we modelled this sensor as a non-linear measurement of true state $\vec{x}_t$ plus a white Gaussian noise as shown in Equation (\ref{f17}). This sensor in our case should satisfy the following conditions:
\begin{itemize}
\item {This sensor should carry rich information as we need to obtain accurate camera pose estimation from it.}
\item {This sensor, unlike GPS, should work both inside and outside environment as our system is designed for mobile robots.}
\item {This sensor should be light-weight, low-expense and better easy to handle as this is the general requirements for mobile robots.}
\end{itemize}
Above conditions lead to our choice --- camera. It is worthy to note that though we treat data in correction step as a black box, it is also possible to choose a multiple sensory platform. However on the one hand, we choose camera not only it satisfies all the above conditions, also the camera as the most commonly-used sensor has been well-investigated and well-understand. It is more convenient to find multiple possible approaches to solve our issues, which is precisely camera pose estimation in our case; on the other hand, multiple extra sensors (\ie, camera+GPS) probably meet synchronization issues. Thereby we choose the camera images as complementary data in this work.

\subsection{Introduction to Monocular Visual Odometry}
\label{subsec:camera_comple_data_sub1}

Visual odometry estimates the camera pose (\eg, global translation and rotation) we require in ESKF prediction step as the transformation between camera and IMU usually has been pre-calibrated. Though there are other systems such as stereo-based odometry \cite{mei2009constant}, depth-camera odometry \cite{newcombe2011kinectfusion} in visual odometry categories, however considering we only use single camera in this work, we here mainly introduce different types of monocular visual odometry.

A monocular odometry usually works as follows. Initially, the system obtains the first estimation of camera's pose by \textit{Homography} \cite{wiki:Homography} between two images. After a new image is acquired, the system then tries to find the correspondences among common \textit{landmarks} in different views, where a landmark is defined as the most distinctive object in the real world. 

Depending on the types of correspondences, we have
\begin{itemize}
\item {\textbf{Feature-based methods} As proposed in \cite{davison2007monoslam, eade2007monocular, klein2007parallel, mur2015orb}, feature-based methods use image features to denote the correspondences among landmarks, \eg, 3D points belongs to the same if the features corresponding to it are same. The system then reprojects similar points from the last image to the current one using estimated camera pose transformation, and errors between points in the current visual frame is called \textit{reprojection error}.}
\item {\textbf{Direct methods} As proposed in \cite{engel2014lsd}, it uses image intensity (\eg, photometric error) to find such correspondences.}
\item {\textbf{Semi-direct methods} As shown in \cite{forster2014svo}, it uses combinations of image features and image intensities to find correspondences.}
\end{itemize}
After the connections of points between two images have been established, camera pose is estimated by minimizing the re-projection error and/or image intensity error.

Building a map based on such a odometry is rather straightforward. A point is recognized as a part of landmark if it has been frequently tracked, and then it will be inserted into the map as a recognized \textit{map point}; a map is then constructed by these map points, and normally a batch processing (\eg, bundle adjustment) is used for optimizing the map during mapping process. However, such a technique can also be used to improve the pose estimation quality as we discuss in Section \ref{subsec:camera_comple_data_sub3}.

\subsection{Self-adapt Map Scale}
\label{subsec:camera_comple_data_sub2}

The estimation of camera pose from monocular visual odometry (VO) is usually a good compensation to ESKF IMU integration since global translation is unobservable to IMU integration. Though camera is an angle-sensor, it is impossible to obtain the real map scale \cite{forster2014svo} since the global translation from monocular VO has been scaled up/down together with real world scale. Researchers have tried to estimate map scale factor by aligning the first few frames with ground-truth data \cite{forster2014svo}, or initialize map scale by a standard object (\eg, a A4 paper) \cite{davison2007monoslam}, but it is still likely to accumulate scale drift with time goes on. In our framework, since IMU measures under the real world scale, we can propagate the map scale in our ESKF framework by introducing a scale factor in our state representations. 

We first add the scale factor $\vec{\lambda}$ which contains a 3-vector to represent the scalability with x-axis, y-axis, and z-axis respectively. The advantage of 3-vector rather than a scalar is that we can somehow avoid the crazy jumps even if the differences between two successive measurements is very small in certain axis, \ie, a micro helicopter flies with a unchanged height. Now our true state, nominal state, and error state have been changed into
\begin{align}
	\vec{x}_t &= \left[ \vec{p}_t \ \vec{v}_t \ \vec{q}_t \ \vec{a}_{bt} \ \vec{\omega}_{bt} \ \vec{g}_{t} \ \vec{\lambda}_{t} \right]^T \\
	\vec{x}_n &= \left[ \vec{p}_n \ \vec{v}_n \ \vec{q}_n \ \vec{a}_{bn} \ \vec{\omega}_{bn} \ \vec{g}_{n} \ \vec{\lambda}_{n} \right]^T \\
	\vec{x}_e &= \left[ \vec{p}_e \ \vec{v}_e \ \vec{\theta}_e \ \vec{a}_{be} \ \vec{\omega}_{be} \ \vec{g}_{e} \ \vec{\lambda}_{e} \right]^T .
\end{align}
The derivative with $\vec{\lambda}$ with respect to time can be easily derived since we assume the scale factor is independent with time, therefore we have
\begin{align}
	\dot{\vec{\lambda}_t} &= 0 \\
	\dot{\vec{\lambda}_n} &= 0 \\
	\dot{\vec{\lambda}_e} &= 0.
\end{align}
It is trivial to add map scale factors to system kinematic equations (\eg, Equation (\ref{f18}) to (\ref{f19})) and the error state Jacobian (\eg, Equation (\ref{f15}) and (\ref{f16})). The measurement function $h(\cdot)$ of estimated true state $\hat{\vec{x}_t}$ can be derived as
\begin{align}
	\label{f23}
	h(\vec{p}_t) &= \vec{\lambda}_t \odot (\vec{p}_t - \vec{p}_{t0}) + \vec{p}_{t0}\\
	\label{f24}
	h(\vec{v}_t) &= \vec{v}_t\\
	\label{f25}
	h(\vec{q}_t) &= \vec{q}_t\\
	\label{f26}
	h(\vec{a}_{bt}) &= \vec{a}_{bt} \\
	\label{f27}
	h(\vec{\omega}_{bt}) &= \vec{\omega}_{bt} \\ 
	\label{f28}
	h(\vec{g}_t) &= \vec{g}_t \\
	\label{f29}
	h(\vec{\lambda}_t) &= \vec{\lambda}_t，
\end{align}
where $\odot$ is point-wise vector multiplication and $\dot{\vec{p}_{t0}}$ is the camera/IMU position in reference frame, \ie, the position obtained from Homography of the first two keyframes. For simplification, we hereby assume the camera sensor and IMU sensor have identical position, \ie, $\dot{\vec{p}_t}$ is the estimated camera position in world frame. A solution to more complicated cases can be found in \cite{lynen2013robust}.

In Section \ref{sec:ESKF_IMU}, we have not given the explicit expression of $\mH$ in Equation (\ref{f22}). $\mH$ is defined as the Jacobian matrix of extrasensory measurement function $h(\cdot)$ with respect to the error state at nominal state $\vec{x}_n$, as $\vec{x}_n$ is the estimation of true state $\vec{x}_t$ here. By chain rule, $\mH$ can be written as
\begin{equation}
	\mH \triangleq \frac{\partial{h}}{\partial{\vec{x}_e}}\Bigr|_{\vec{x}_n} = \frac{\partial{h}}{\partial{\vec{x}_t}}\Bigr|_{\vec{x}_n}\frac{\partial{\vec{x}_t}}{\partial{\vec{x}_e}}\Bigr|_{\vec{x}_n} = \mJ_H\mJ_{t e}.
\end{equation}
We have already given $\mJ_{t e}$ in Equation (\ref{f15}), we then derive $\mJ_H$, which leads to
\begin{equation}
	\mJ_H = \begin{bmatrix}
	\mJ_p & 0 & 0 & 0 & 0 & 0 & 0\\
	0 & \ones_3 &0 & 0 & 0 & 0 & 0\\
	0 & 0 & \ones_4 & 0 & 0 & 0 & 0\\
	0 & 0 & 0 & \ones_3 & 0 & 0 & 0\\
	0 & 0 & 0 & 0 & \ones_3 & 0 & 0\\
	0 & 0 & 0 & 0 & 0 & \ones_3 & 0\\
	0 & 0 & 0 & 0 & 0 & 0 & \ones_3 \\
	\end{bmatrix},
\end{equation}
where $\mJ_p$ is a $3 \times 3$ matrix with diagonal being the three elements of $\vec{\lambda}_n$ respectively.

\subsection{Keyframe-based Bundle Adjustment}
\label{subsec:camera_comple_data_sub3}

At last, a keyframe-based bundle adjustment is used to further increase pose estimation accuracy.

A general bundle adjustment tries to optimize bunch of camera poses and 3D points together by minimizing the reprojection error between reprojected 3D points and predicted image points. Mathematically, we obtain our optimized camera poses vector $\vec{c}$ and 3D points vector $\vec{p}$ by
\begin{equation}\label{c1}
	\{ \vec{c}, \vec{p} \} = \argmin_{\vec{c}_i, \vec{p}_j} \sum_{i=1}^n \sum_{j=1}^m Obj(CamProj(\vec{c}_i, \vec{p}_j), \mI_{ij})^2,
\end{equation}
where $n$ and $m$ are the number of camera poses and visible 3D points respectively, $\mI_{ij}$ is the predicted 2D point position corresponding to $j^{th}$ visible 3D point in $i^{th}$ image frame, function $CamProj(\cdot)$ reprojects the visible 3D point from global frame into camera frame and $Obj(\cdot)$ is a general function (\eg, Euclidean distance) that measures the error between two 2D points.

As long as we assume our camera is pin-hole model, we can give function $CamProj(\cdot)$ as
\begin{align}
	\label{c2}
	CamProj(\vec{c}_i, \vec{p}_j) &= K (\mR \{ \vec{c}_i \} \vec{p}_j) \\
	K(\vec{P}_\C) &= \left[ u_0 \ v_0 \right]^T + \begin{bmatrix} f_u & 0 \\
															 0 & f_v
											  \end{bmatrix} r \left[ \frac{P_x}{P_z} \ \frac{P_y}{P_z} \right]^T,
\end{align}
where $\mR \{ \vec{c}_i \}$ is the rotation matrix corresponding to camera pose, function $K(\cdot)$ transforms a point $P$ in camera frame $\C$ into 2D point in image plane. Parameters $u_0$, $v_0$ are principle point, $f_u$, $f_v$ are focal length, and $r$ is the distortion factor, where these parameters are obtained by the camera calibration.

After correction step, we employ such bundle adjustment (BA) step at each keyframe by inputting the nominal state camera poses and 3D points; we will apply an extra correction step using the BA results to further improve the estimation quality. In order to ensure the efficiency, we have ignored the oldest key frame after our key frame queue has been reached maximum number, we set this number to 20 in all our experiments. We have observed this number is suitable and keep our visual-inertial odometry runs in real-time, while improving the estimation accuracy at the same time.

Solutions to general bundle adjustment varies \cite{wu2011multicore, triggs1999bundle, lourakis2009sba}, we have mainly follow the work by \cite{wu2011multicore} since it has been shown efficient for sparse bundle adjustment problem.

\section{Visual-inertial Odometry Pipeline Summary}
\label{sec:pipeline_summary}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.5\textwidth]{CONTENT/Figure/Figure4-1_Pipeline.png}
    \caption{Pipeline of our visual-inertial odometry. Noted that measurement frequency from camera is 4 times lower than measurement frequency of IMU sensor in our experiment setting, therefore we do not have visual odometry (VO) result in some certain turn. Also whether this frame is keyframe is provided by VO.}
    \label{fig:fig4-1}
\end{figure}


In this section we summarize our visual-inertial odometry (VIO) pipeline (see Figure \ref{fig:fig4-1}). We also analyses the computational cost of our visual inertial odometry system in this section.

An initialisation step is necessary for both nominal state and error state. After the system obtains the measurements (\eg, 3-vector from accelerometer, and 3-vector from gyroscope) from the IMU sensor, nominal state and error state are then predicted using Equation (\ref{f112}) to (\ref{f114}). The system updates corresponding covariance matrix using Equation (\ref{f30}). 

If the camera gives an estimated camera pose in this turn, the system applies a \textit{correction step} as we have described in Section \ref{subsec:ESKF_IMU_sub4}. After injecting error state from nominal state, we obtain the estimated true state. System then reset the error state, and further improving the odometry estimation quality by applying a keyframe bundle adjustment (keyframe BA) if this camera frame is specified as key frame by visual odometry as we have discussed in Section \ref{subsec:camera_comple_data_sub3}. Finally the system outputs the estimated camera pose and starts the next round after having received IMU measurements.

In our ESKF framework, computational complexity remains constant since we only keep the current states and covariance, and the size of states and covariance matrix is fixed during estimation process. In order to verification, we have demonstrated an experiment (see Section \ref{subsec:experiment1}) to show that computational time approximately remains unchanged when running a single IMU integration process using ESKF. As we have discussed in Section \ref{sec:FVK}, the computational complexity of keyframed-based visual odometry is 
\begin{equation}
	O(m^2 \cdot n),
\end{equation}
where $m$ is the number of key frame, and $n$ is the number of landmarks. We claim that our odometry can also be extended to large scale since the number of landmarks have no impact on our time consuming and this is in general the key factor of limiting the scalability of SLAM-like system. Moreover \cite{strasdat2010real} have shown that it is more beneficial to obtain high estimation accuracy by increasing the number of landmarks than number of key frames.

We are not able to build an environmental map due to the time limitation of this master thesis. Although a few more map optimization steps are needed, the key frame BA we have proposed in Section \ref{subsec:camera_comple_data_sub3} gives a direct result of optimized landmarks, which are the main components of mapping.

In summary, in this chapter we suggest an error-state Kalman filter (ESKF) based visual-inertial odometry. The system accurately estimates the camera-IMU platform pose by fusing the measurements from the visual-inertial sensors. Using a loosely-coupled approach, the system runs ESKF in a constant computational complexity and needs no special initialization steps, therefore it is suitable for mobile robot localization in real-time. This odometry can also be extended to an efficient and scalable SLAM system.

