%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Background on Multilinear Algebra}
\label{chap:background}
Tensors lie at the center of hypergraph matching problems as they are used to represent higher-order affinities between groups of vertices.
Multilinear forms, meanwhile, allow us to define the objective function of hypergraph matching problems in an abstract way.
Thus, we present in this chapter a fundamental knowledge of tensors and multilinear algebra that is required for understanding hypergraph matching
as well as our work in this thesis.

\section{Tensor}
\label{sec:tensor}
In this section, we give the definition of tensor and present some operations on tensor such as their multiplications with vectors and matrices.
As we will see in next chapters, tensors play a crucial role in the formulation of hypergraph matching problems 
where they are used to represent hyperedges of hypergraphs or higher-order affinities between tuples of points.

\subsection{Definitions}
\begin{definition}[Tensor]\label{def:tensor}
    A tensor $\AC$ of order $m$ is an $m-$way array, which generalizes the notion of a matrix (a $2-$way array). 
    The entries of $\AC$ are accessed via $m$ indices, namely $\AC_{i_1 \ldots i_m}$ for all $1 \leq i_j \leq I_j$ where $1\leq j\leq m$.
    The integer numbers $I_j (1 \leq j \leq m)$ are called the dimensions of the tensor.
\end{definition}
We denote $\RR^{I_1 \times \ldots \times I_m}$ as the set of order-$m$ dimensions-$(I_1, \ldots, I_m)$ real tensors.
When all the dimensions of the tensor are the same, i.e. $I_1 = \ldots = I_m = n$, this notation can be simplified to $\RR^{n^m}$.
\begin{definition}[Symmetric Tensor]\label{def:symTensor}
    A tensor $\AC \in \RR^{n^m}$ is symmetric if its entries are invariant under any permutation of its indices, i.e.
     $$
	  \AC_{i_{\sigma(1)} \ldots i_{\sigma(m)}} = \AC_{i_1 \ldots i_m} \quad \forall i_1, \ldots, i_m \in [1\ldots n], \sigma \in \mathfrak{G}_m
     $$
     where $\mathfrak{G}_m$ denotes the permutation group of $\{1,2,\ldots,m\}$.
\end{definition}

\subsection{Tensor Operations}
\begin{definition}[Tensor scalar product]\label{def:tensorScaProd}
The scalar product between two tensors $\AC, \BC \in \RR^{I_1 \times \ldots \times I_m}$ is given as
$$
    \<\AC, \BC\> = \multsum \AC_{i_1 \ldots i_m} \BC_{i_1 \ldots i_m} .
$$    
\end{definition}

\begin{definition}[Frobenius norm of a tensor]\label{def:froNorm}
The Frobenius norm of a tensor $\AC \in \RR^{I_1 \times \ldots \times I_m}$ is defined as
$$
    \norm{\AC}_F = \sqrt{\< \AC, \AC \>} = \sqrt{ \multsum \AC_{i_1 \ldots i_m}^2 } .
$$    
\end{definition}

\begin{definition}[Tensor product]\label{def:tensorVecProd}
The tensor product between two tensors $\AC \in \RR^{I_1 \times \ldots \times I_m}$ and  $\BC \in \RR^{J_1 \times \ldots \times J_n}$, 
denoted as $\AC \otimes_{\TC} \BC$,
is a tensor in $\RR^{I_1 \times \ldots \times I_m \times J_1 \ldots \times J_n}$ such that its entries are given as
$$
      {(\AC \otimes_{\TC} \BC)}_{i_1 \ldots i_m j_1 \ldots j_n} = \AC_{i_1 \ldots i_m} \BC_{j_1 \ldots j_n}
$$ for all $1 \leq i_k \leq I_k$ with $1 \leq k \leq m$ and $1 \leq j_l \leq J_l$ with $1 \leq l \leq n.$
\end{definition}

\begin{definition}[Tensor-matrix product]\label{def:tensorMatProd}
The mode-n product between a tensor $\AC \in \RR^{I_1 \times \ldots \times I_m}$ 
and a matrix $\bX \in \RR^{J_n \times I_n}$ is a tensor in $\RR^{I_1\times\ldots\times I_{n-1}\times J_n\times I_{n+1}\times\ldots\times I_m}$ such that
$$
      {(\AC \otimes_n \bX)}_{i_1\ldots i_{n-1} j_n i_{n+1} \ldots i_m} = \sum_{i_n=1}^{I_n} \AC_{i_1 \ldots i_m} \bX_{j_n i_n}
$$ for all $1\leq i_k \leq I_k$ with $1 \leq k \leq m, k \neq n$ and $1 \leq j_n \leq J_n.$
\end{definition}

The multiplication between a tensor and a vector presented below is helpful in representing multilinear forms which are defined in the next section.
\begin{definition}[Tensor-vector product]\label{def:tensorVecProd}
The mode-n product between a tensor $\AC \in \RR^{I_1 \times \ldots \times I_m}$ 
and a vector $\bx \in \RR^{I_n}$ is a tensor in $\RR^{I_1 \times \ldots \times I_{n-1} \times I_{n+1} \ldots \times I_m}$ such that
$$
      {(\AC \otimes_n \bx)}_{i_1 \ldots i_{n-1} i_{n+1} \ldots i_m} = \sum_{i_n=1}^{I_n} \AC_{i_1 \ldots i_m} \bx_{i_n}
$$ for all $1\leq i_k \leq I_k$ with $1 \leq k \leq m, k \neq n.$
\end{definition}

\section{Multilinear Form}
\label{sec:mult_form}
In this section, we give the definition of multilinear form and some standard notations.
Multilinear forms are helpful in representing the objective function of hypergraph matching problems 
as presented in Section \ref{sec:GM_HAP_formulations}.

\subsection{Definitions}
\begin{definition}[Multilinear form]\label{def:multForm}
    Let $V_1, \ldots, V_m$ and $U$ be vector spaces. A form
    $$
	\MF: V_1 \times \cdots \times V_m \to U, \quad (\bx^1, \ldots, \bx^m) \mapsto \MF(\bx^1, \ldots, \bx^m)
    $$
    is multilinear if it is linear in each $\bx^i$ for every $1 \leq i \leq m.$
\end{definition}

From Definition \ref{def:tensorVecProd}, we can associate to a tensor a multilinear form and vice versa.
In fact, given a tensor $\FC \in \RR^{I_1 \times \ldots \times I_m}$, one can obtain the multilinear form 
$\MF: \RR^{I_1} \times \cdots \times \RR^{I_m} \to \RR$ such that
$$
      \MF(\bx^1,\ldots,\bx^m) \bydef \FC \otimes_1 \bx^1 \otimes_2 \ldots \otimes_m \bx^m = \multsum \FC_{i_1 \ldots i_m} \bx^1_{i_1} \ldots \bx^m_{i_m} .
$$
When $\FC$ is a square tensor, that is, $I_1 = \cdots = I_m$, and all the argument vectors are the same, i.e. $\bx^1=\cdots=\bx^m=\bx$, 
then the multilinear form associated to $\FC$ induces an $m$-degree homogeneous polynomial function, namely $\MF(\bx,\ldots,\bx)$.
We will see in Section \ref{sec:GM_HAP_formulations} of Chapter \ref{chap:review} 
that $\MF(\bx,\ldots,\bx)$ is the objective function of $m$-uniform hypergraph matching problems.

So far, there are two ways one can think of tensors: 
\begin{itemize}
    \item tensors are elements of a ``tensor product'' of two or more vector spaces
    \item tensors are multilinear forms
\end{itemize}
While the first way is still abstract, the second is more concrete and powerful as we will see in the next chapters.

\begin{definition}[Symmetric multilinear form]\label{def:symMultForm}
    Let $V_1, \ldots, V_m$ and $U$ be vector spaces and $\MF$ be a multilinear form
    $$
	\MF: V_1 \times \cdots \times V_m \to U, \quad (\bx^1, \ldots, \bx^m) \mapsto \MF(\bx^1, \ldots, \bx^m) .
    $$
    Then $\MF$ is called symmetric if $\MF(\bx^1, \ldots, \bx^m) = \MF(\bx^{\sigma(1)}, \ldots, \bx^{\sigma(m)})$
    for all $\sigma \in \mathfrak{G}_m$ and $(\bx_1,\ldots,\bx_m) \in V_1\times\cdots\times V_m.$
\end{definition}
It can be seen that the multilinear form associated to a symmetric tensor is symmetric.

% One can show that the multilinear form associated to a symmetric tensor is symmetric.
% In fact, it holds for all $\sigma \in \mathfrak{G}_m$
% \begin{eqnarray*}
%     \MF(\bx^1,\ldots,\bx^m) 
%     &\bydef& \FC \otimes_1 \bx^1 \otimes_2 \ldots \otimes_m \bx^m \\
%     &=& \multsum \FC_{i_1 \ldots i_m} \bx^1_{i_1} \ldots \bx^m_{i_m} \\
%     &=& \sum_{i_1=1}^{I_1} \sum_{i_2=i_1}^{I_2} \cdots \sum_{i_m=i_{m-1}}^{I_m}  \sum_{\pi \in \Pi\{i_1,\ldots,i_m\}}
%     \MF_{\pi(1) \ldots \pi(m)} \bx^1_{\pi(1)} \ldots \bx^m_{\pi(m)} \\
%     &=& \sum_{i_1=1}^{I_1} \sum_{i_2=i_1}^{I_2} \cdots \sum_{i_m=i_{m-1}}^{I_m}  \MF_{i_1 \ldots i_m} \sum_{\pi \in \Pi\{i_1,\ldots,i_m\}}
%     \bx^1_{\pi(1)} \ldots \bx^m_{\pi(m)} \\
%     &=& \sum_{i_1=1}^{I_1} \sum_{i_2=i_1}^{I_2} \cdots \sum_{i_m=i_{m-1}}^{I_m}  \MF_{i_1 \ldots i_m} \sum_{\pi \in \Pi\{i_1,\ldots,i_m\}}
%     \bx^{\sigma(1)}_{\pi(1)} \ldots \bx^{\sigma(m)}_{\pi(m)} \\
%     &=& \sum_{i_1=1}^{I_1} \sum_{i_2=i_1}^{I_2} \cdots \sum_{i_m=i_{m-1}}^{I_m}  \sum_{\pi \in \Pi\{i_1,\ldots,i_m\}}
%     \MF_{\pi(1) \ldots \pi(m)} \bx^{\sigma(1)}_{\pi(1)} \ldots \bx^{\sigma(m)}_{\pi(m)} \\
%     &=& \MF( \bx^{\sigma(1)}, \ldots, \bx^{\sigma(m)} ) \\    
% \end{eqnarray*} where $\Pi\{i_1,\ldots,i_m\}$ is the set of distinct permutations of $\{i_1,\ldots,i_m\}.$

% \subsection{Multilinear Operations}
\subsection{Notations}
Consider a 4th order tensor $\FC \in \RR^{I\times J\times K\times L}$ and its associated multilnear form $\MF(\bx,\by,\bz,\bt)$,
then we denote $\MF(\bx,\by,\bz,\cdot)$ as the gradient of the multilinear function with respect to $\bt$, that is, 
$\MF(\bx,\by,\bz,\cdot) \bydef \bu \in \RR^L$ s.t. 
$\bu_l = \sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K \FC_{ijkl} \bx_i \by_j \bz_k$ for all $1\leq l \leq L$.
Thus, one can write $\MF(\bx,\by,\bz,\bt) = \inner{\MF(\bx,\by,\bz,\cdot), \bt}$.
Note that if $\FC$ is symmetric then one can permute the arguments without changing the function, and hence, 
the position of the dot in $\MF(\bx,\by,\bz,\cdot)$ does not matter.
% that is, $\MF(\bx,\by,\bz,\cdot) = \MF(\bx,\by,\cdot, \bz) = \MF(\bx,\cdot,\by,\bz) = \MF(\cdot,\bx,\by,\bz)$.

In the same fashion, we denote $\FC(\bx,\by,\cdot,\cdot)$ as a matrix in $\RR^{I\times J}$:
$\FC(\bx,\by,\cdot,\cdot) \bydef \bA \in \RR^{K\times L}$ s.t. $\bA_{kl} = \sum_{i=1}^I \sum_{j=1}^J \FC_{ijkl} \bx_i \by_j$.
Thus, one can write $\MF(\bx,\by,\bz,\bt) = \inner{\bz, \MF(\bx,\by,\cdot,\cdot) \bt}$.
Again, if $\MF$ is symmetric then the order of the arguments can be exchanged.

% \subsection{The Induced Functions of a Multilinear Form}
% Apart from the bijection between a multilinear form and a tensor, we can also associate to a multilinear form many different functions.
% In this thesis, we call them the \textit{induced functions} of a multilnear form.
% 
% \begin{definition}\label{def:induced_functions_of_multilinear_form}
%     Given a multilinear form $\MF: \RR^{n^m} \to \RR$ and a set of argument vectors $\bx^1, \bx^2, \ldots, \bx^m \in \RR^n$.
%     Then $\MF$ induces the following functions
%     $$
% 	\MF(\bx^{i_1}, \bx^{i_2}, \ldots, \bx^{i_m})
%     $$ for any tuple $(i_1, i_2, \ldots, i_m)$ such that $1 \leq i_1, i_2, \ldots, i_m \leq m.$
% \end{definition}
% 
% Note that there are some redundancy in the above definition.
% In fact, one can view the functions 
% $$
%     \MF(\bx^1,\ldots,\bx^1), \MF(\bx^2,\ldots,\bx^2),\ldots,\MF(\bx^m,\ldots,\bx^m)
% $$ as a unique one because they all represent the same function, say $\MF(\bx,\ldots,\bx)$, when all the argument vectors are the same.
% This applies for the other cases as well where some of the arguments are repeated.
% 
% To clarify the definition, we take a 4th order symmetric multilinear form $\MF$ for example. 
% Then, all the induced function can be listed as
% $$
%     \MF(\bx,\bx,\bx,\bx), \MF(\bx,\bx,\bx,\by), \MF(\bx,\bx,\by,\by), \MF(\bx,\bx,\by,\bz), \MF(\bx,\by,\bz,\bt).
% $$
% Note that the ordering of argument vectors does not matter due to the supersymmetry of $\MF$.
